Homework 6
================
Christian Pascual
11/15/2018

Bringing in the usual suspects
==============================

``` r
library(tidyverse)

knitr::opts_chunk$set(
  out.width = "90%"
)

theme_set(theme_classic() + 
            theme(legend.position = "bottom", 
                  plot.title = element_text(hjust = 0.5)))
```

Bringing in tidyverse to do the heavy lifting with data cleaning, analysis and plotting.

Problem 1
=========

Data Wrangling
--------------

First, we need to bring in the data for analysis.

``` r
homicides = read.csv(file = "./data/homicide-data.csv") 
```

Like in the last homework, we'll create a `city-state` variable to hold a more descriptive name for each city. We'll need to filter out some cities that don't report race and then alter some columns on the victim characteristics.

``` r
cities_to_remove = c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO")
clean_homicides = homicides %>% 
  mutate(city_state = paste(city, state, sep = ", "),
         is_white = ifelse(victim_race == "White", "white", "nonwhite"),
         victim_race = fct_relevel(is_white, "white"),
         victim_age = as.numeric(victim_age),
         was_resolved = ifelse(disposition == "Closed by arrest", 1, 0)) %>% 
  filter(!(city_state %in% cities_to_remove))
```

Our cleaning results in a dataset that is 48508 rows and 15 columns. This dataset will be used in our regression analyses.

Logistic Regression on Baltimore
--------------------------------

With this data, we'll use `glm` to create a logisitic regression to predict if a crime will be solved based on the victim's age, sex and race.

``` r
baltimore_glm = clean_homicides %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(was_resolved ~ victim_age + victim_sex + victim_race, 
      data = ., family = binomial())
```

With this model on hand, we can calculate the odds ratios needed for the problem.

``` r
coeffs = broom::tidy(baltimore_glm) %>% 
  mutate(OR = exp(estimate)) 

knitr::kable(coeffs)
```

| term                 |    estimate|  std.error|  statistic|    p.value|         OR|
|:---------------------|-----------:|----------:|----------:|----------:|----------:|
| (Intercept)          |   1.0473606|  0.2265545|   4.622996|  0.0000038|  2.8501185|
| victim\_age          |  -0.0037403|  0.0030328|  -1.233272|  0.2174741|  0.9962667|
| victim\_sexMale      |  -0.8845347|  0.1361009|  -6.499110|  0.0000000|  0.4129062|
| victim\_racenonwhite |  -0.7929220|  0.1742183|  -4.551312|  0.0000053|  0.4525206|

From the table, we can see that homicide cases among non-whites are much less likely to be solved compared to cases among white people. The adjusted odds ratio is 0.453 with a p-value of 5.331248210^{-6}, indicating statistically significant difference on a 95% confidence level.

All City Regression
-------------------

The above analysis lays out the groundwork for applying it to all the cities as a whole. From the orignial `clean_homicides` dataset, we'll create a new tibble that also contains the odds ratios and confidence intervals. Since both the estimates are in terms of logarithms, we need to transform both the estimate odds ratio and confidence intervals back.

``` r
# Function to apply the logit to all cities
resolve_logit = function(df) {
  return(glm(was_resolved ~ victim_age + victim_sex + victim_race, 
            data = df, family = binomial()))
}

city_odds = clean_homicides %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(model = map(data, resolve_logit),
         coeffs = map(model, broom::tidy)) %>% 
  unnest(coeffs) %>% 
  filter(term == "victim_racenonwhite") %>% 
  mutate(OR = exp(estimate),
         conf.low = exp(estimate - std.error * 1.96),
         conf.high = exp(estimate + std.error * 1.96)) %>% 
  select(city_state, OR, conf.low, conf.high)
```

The above pipeline produces a tidy dataset, complete with the estimated adjusted odds ratios and associated confidence intervals. Now we can plot these and see how it ranges between the cities in the Washington Post dataset.

``` r
ggplot(data = city_odds, aes(x = reorder(city_state, -OR), y = OR,
                             color = reorder(city_state, -OR))) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  geom_hline(yintercept = 1, alpha = 0.3) +
  theme(legend.position = "none", 
        axis.text.x = element_text(angle = 60, hjust = 1),
        text = element_text(size = 10)) +
  labs(
    title = "Adjusted odds ratios for homicide resolution\n comparing non-whites to whites by city-state pairs",
    x = "City, State",
    y = "Adjusted odds ratios"
  )
```

<img src="P8105_HW6_cbp2128_files/figure-markdown_github/all-city-plot-1.png" width="90%" />

With the exception of Tampa, Durham and Birmingham, most of the cities in the dataset have point estimates for adjusted odds ratios less than 1. In fact, the cities with the largest adjusted odds ratios seem to have the largest confidence intervals, suggestive of relatively small presence in `clean_homcides`. Any adjusted odds ratio under 1 implies that less homicides are solved for non-whites compared to whites.

The odds ratios are sorted in descending order for easier visual comparison. The 95% confidence intervals show that not as many of the point estimates do not rule out the null hypothesis that `aOR = 1`. I've added a line at `OR = 1` for emphasis. Taking the confidence intervals into account, it looks that city-states don't start looking significantly biased against non-whites until Jacksonville.
